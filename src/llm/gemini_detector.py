"""
Gemini 2.5 Flash + Rule-based Filter í†µí•© ì‹œìŠ¤í…œ
ë¹ ë¥´ê³  ì €ë ´í•˜ë©° ì •í™•í•œ ë‹¨ì¼ LLM ì†”ë£¨ì…˜
"""
import logging
from typing import Dict, Optional
from src.llm.llm_clients.gemini_client import GeminiClient
from src.filters.rule_filter import RuleBasedFilter

logger = logging.getLogger(__name__)


class GeminiPhishingDetector:
    """
    Gemini 2.5 Flash + Rule-based Filter ì¡°í•©
    - ë¹ ë¥¸ ì‘ë‹µ ì†ë„
    - ì €ë ´í•œ ë¹„ìš© (ë¬´ë£Œ í‹°ì–´)
    - 96.3% ê¸°ë³¸ ì •í™•ë„ + Rule Filterë¡œ 98%+ ëª©í‘œ
    """

    def __init__(self):
        self.gemini = GeminiClient()
        self.rule_filter = RuleBasedFilter()
        self.model_name = "Gemini 2.5 Flash + Rule Filter"

        if not self.gemini.is_available():
            logger.warning("Gemini API key not configured")
        else:
            logger.info("âœ“ Gemini Phishing Detector initialized")

    def is_available(self) -> bool:
        """Gemini API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.gemini.is_available()

    def analyze(self, text: str, enable_filter: bool = True) -> Dict:
        """
        ë³´ì´ìŠ¤í”¼ì‹± ë¶„ì„ (Gemini + Rule Filter)

        Args:
            text: í†µí™” ë‚´ìš©
            enable_filter: Rule Filter ì ìš© ì—¬ë¶€ (ê¸°ë³¸: True)

        Returns:
            {
                "score": ìµœì¢… ì ìˆ˜ (0-100),
                "risk_level": ìœ„í—˜ë„,
                "is_phishing": í”¼ì‹± ì—¬ë¶€,
                "reasoning": íŒì • ì´ìœ ,
                "model": ëª¨ë¸ëª…,
                "filter_applied": í•„í„° ì ìš© ì—¬ë¶€,
                "llm_score": ì›ë³¸ LLM ì ìˆ˜,
                "keyword_analysis": í‚¤ì›Œë“œ ë¶„ì„
            }
        """
        if not self.is_available():
            return self._error_response("Gemini API not configured")

        try:
            # Step 1: Gemini ë¶„ì„
            logger.info(f"ğŸ” Gemini analyzing: {text[:50]}...")

            prompt = self._build_prompt()
            gemini_result = self.gemini.analyze_phishing(text, prompt)

            llm_score = gemini_result.get("score", 50)
            llm_reasoning = gemini_result.get("reasoning", "")

            # Step 2: Rule Filter ì ìš© (í•­ìƒ ì‹¤í–‰í•´ì„œ í‚¤ì›Œë“œ ë¶„ì„ ì–»ê¸°)
            filter_result = None
            if enable_filter:
                logger.info("âš™ï¸ Applying Rule-based Filter...")
                filter_result = self.rule_filter.filter(
                    text=text,
                    llm_score=llm_score,
                    llm_reasoning=llm_reasoning
                )

                final_score = filter_result["final_score"]
                filter_applied = filter_result["filter_applied"]
                # í•­ìƒ keyword_analysis ê°€ì ¸ì˜´ (í•„í„° ì ìš© ì—¬ë¶€ì™€ ë¬´ê´€)
                keyword_analysis = filter_result.get("keyword_analysis", {})

                # í•„í„°ê°€ ì ìš©ë˜ì—ˆìœ¼ë©´ ë¡œê·¸
                if filter_applied:
                    logger.info(
                        f"âœ“ Rule Filter {'downgraded' if final_score < llm_score else 'upgraded'}: "
                        f"{llm_score} â†’ {final_score} ({filter_result['reason']})"
                    )
            else:
                final_score = llm_score
                filter_applied = False
                keyword_analysis = {}

            # Step 3: ìµœì¢… ìœ„í—˜ë„ íŒì •
            risk_level, is_phishing = self._calculate_risk(final_score)

            # íƒì§€ëœ í”¼ì‹± ê¸°ë²• ì¶”ì¶œ (í•­ìƒ í‘œì‹œ)
            detected_techniques = filter_result.get("detected_techniques", []) if filter_result else []

            # Component scores ê³„ì‚° (ì›ë˜ ì‹œìŠ¤í…œ ì ìˆ˜ ë³µì›)
            component_scores = {}
            if keyword_analysis:
                # ë²”ì£„ í‚¤ì›Œë“œ ì ìˆ˜: 0-10ê°œ ê¸°ì¤€ â†’ 0-100
                crime_score = min(keyword_analysis.get("crime", 0) * 10, 100)
                # ì •ìƒ í‚¤ì›Œë“œ ì ìˆ˜: ë§ì„ìˆ˜ë¡ ì•ˆì „ â†’ ì—­ì‚° (10ê°œ ê¸°ì¤€)
                legit_score = max(100 - keyword_analysis.get("legit", 0) * 10, 0)
                # ê¸´ê¸‰ì„± í‚¤ì›Œë“œ ì ìˆ˜: 0-10ê°œ ê¸°ì¤€ â†’ 0-100
                urgency_score = min(keyword_analysis.get("urgency", 0) * 10, 100)

                component_scores = {
                    "keyword": crime_score,
                    "sentiment": urgency_score,
                    "similarity": legit_score
                }

            # Reasoning ê²°ì •: Rule Filterê°€ ì ìˆ˜ë¥¼ ë³€ê²½í–ˆìœ¼ë©´ Filterì˜ reasonë§Œ ì‚¬ìš©
            final_reasoning = gemini_result.get("reasoning", "")
            if filter_applied and filter_result and final_score != llm_score:
                # ì ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´ Filter reasonë§Œ í‘œì‹œ (Gemini ì›ë³¸ì€ ìˆ¨ê¹€)
                final_reasoning = filter_result.get("reason", "")

            return {
                "score": final_score,
                "risk_level": risk_level,
                "is_phishing": is_phishing,
                "reasoning": final_reasoning,
                "model": self.model_name,
                "filter_applied": filter_applied,
                "llm_score": llm_score,
                "keyword_analysis": keyword_analysis,
                "component_scores": component_scores,
                "key_points": gemini_result.get("key_points", []),
                "detected_techniques": detected_techniques
            }

        except Exception as e:
            logger.error(f"Gemini Detector error: {e}")
            return self._error_response(str(e))

    def _build_prompt(self) -> str:
        """Geminiìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return """ë‹¹ì‹ ì€ ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í†µí™” ë‚´ìš©ì„ ë¶„ì„í•˜ì„¸ìš”.

**í”¼ì‹± íŒë‹¨ ê¸°ì¤€ (ëª¨ë‘ í•´ë‹¹í•´ì•¼ í”¼ì‹±):**
1. ê³µê³µê¸°ê´€/ê¸°ì—…ì„ **ì‚¬ì¹­**í•˜ë©° + ì•„ë˜ ì¤‘ í•˜ë‚˜ ì´ìƒ:
   - ê¸ˆìœµ ì •ë³´ ìš”êµ¬ (ê³„ì¢Œ, ë¹„ë°€ë²ˆí˜¸, OTP, ì†¡ê¸ˆ)
   - ì•± ì„¤ì¹˜/ì›ê²© ì œì–´ ìš”êµ¬
   - ê°€ì§œ URL ì ‘ì† ìœ ë„ (.com, .net ë“±)
   - ê°œì¸ì •ë³´ ìš”êµ¬ (ì£¼ë¯¼ë²ˆí˜¸, ì¹´ë“œë²ˆí˜¸)
2. ì‹¬ë¦¬ì  ì••ë°• (ê¸´ê¸‰ì„±, ìœ„í˜‘, ì²˜ë²Œ ì–¸ê¸‰)
3. ì¥ì†Œ ì´ë™ ìš”êµ¬ (ëª¨í…”, ë…ë¦½ëœ ê³µê°„ ë“±)

**ì¤‘ìš”: ê³µê³µê¸°ê´€ ì´ë¦„ì´ ë‚˜ì™”ë‹¤ê³  ë¬´ì¡°ê±´ í”¼ì‹±ì´ ì•„ë‹™ë‹ˆë‹¤!**
- ë‹¨ìˆœíˆ "~ì—ì„œ ì „í™”ë“œë ¸ìŠµë‹ˆë‹¤"ë§Œìœ¼ë¡œëŠ” í”¼ì‹± ì•„ë‹˜
- ì¼ë°˜ì ì¸ ì—…ë¬´ í†µí™” (ì˜ˆì•½, ì•ˆë‚´, ë¬¸ì˜)ëŠ” ì •ìƒ
- ê¸ˆìœµ/ê°œì¸ì •ë³´/ì•± ì„¤ì¹˜ ìš”êµ¬ê°€ **ì—†ìœ¼ë©´** ì •ìƒ

**ì •ìƒ ì¼€ì´ìŠ¤ ì˜ˆì‹œ:**
- ë³‘ì›/ìƒë‹´ì„¼í„°ì˜ ì˜ˆì•½ ì•ˆë‚´
- íšŒì‚¬/ì‹ë‹¹ ì˜ˆì•½ ì „í™”
- ê¸°ìˆ ì§€ì› ì„¼í„°ì˜ ì„¤ì • ì•ˆë‚´ (ê³µì‹ ê¸°ëŠ¥ ì‚¬ìš©)
- ì±„ìš© ì•ˆë‚´ ë° ì‹œí—˜ ì¼ì • ê³µì§€

**ì¤‘ìš”:** ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. reasoning í•„ë“œì—ëŠ” ì¤„ë°”ê¿ˆì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

**ì‘ë‹µ í˜•ì‹ (ìœ íš¨í•œ JSONë§Œ ì¶œë ¥):**
{"score": 95, "is_phishing": true, "reasoning": "ê¸ˆìœµê°ë…ì›ì„ ì‚¬ì¹­í•˜ë©° ì•± ì„¤ì¹˜ë¥¼ ìœ ë„í•˜ê³  ê°œì¸ì •ë³´ë¥¼ ìš”êµ¬í•¨"}

ìœ„ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""

    def _calculate_risk(self, score: float) -> tuple:
        """
        ì ìˆ˜ë¥¼ ìœ„í—˜ë„ë¡œ ë³€í™˜

        Returns:
            (risk_level: str, is_phishing: bool)
        """
        if score >= 85:
            return ("ê³ ìœ„í—˜ (ì°¨ë‹¨ ê¶Œì¥)", True)
        elif score >= 70:
            return ("ì¤‘ìœ„í—˜ (ê²½ê³ )", True)
        elif score >= 50:
            return ("ë‚®ì€ ìœ„í—˜ (ì£¼ì˜)", False)
        elif score >= 30:
            return ("ë§¤ìš° ë‚®ìŒ (ì •ìƒ ê°€ëŠ¥ì„±)", False)
        else:
            return ("ì•ˆì „", False)

    def _error_response(self, error: str) -> Dict:
        """ì—ëŸ¬ ì‘ë‹µ"""
        return {
            "score": 50,
            "risk_level": "ì•Œ ìˆ˜ ì—†ìŒ",
            "is_phishing": False,
            "reasoning": f"Error: {error}",
            "model": self.model_name,
            "filter_applied": False,
            "llm_score": 50,
            "keyword_analysis": {},
            "key_points": []
        }

    def get_filter_statistics(self) -> Dict:
        """Rule Filter í†µê³„ ë°˜í™˜"""
        return self.rule_filter.get_statistics()

    def reset_filter_statistics(self):
        """Rule Filter í†µê³„ ì´ˆê¸°í™”"""
        self.rule_filter.reset_statistics()
